{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80aae9a1-a5a3-436e-8825-fcc6513c0e67",
   "metadata": {},
   "source": [
    "# G-Eval result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9f5ca-574c-4b4b-868d-74ab8bd49f2c",
   "metadata": {},
   "source": [
    "## load G-eval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757e4fd-ff7f-4c1f-a114-63bb928db558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "metrics_files = {\n",
    "    'llama3-8b': 'data/ft-v4.1/eval4/llama3-8b-0shot_metrics.json',\n",
    "    'llama3-70b': 'data/ft-v4.1/eval4/llama3-70b-0shot_metrics.json',\n",
    "    'llama3_8b-ft': 'data/ft-v4.1/eval4/llama3_v4.1_10p_lora_a256r512_metrics.json',\n",
    "    # 'llama3_8b-20p-lora': 'data/ft-v4.1/eval4/llama3_v4.1_20p_lora_a256r512_metrics.json',\n",
    "    'llama3-8b-5shot': 'data/ft-v4.1/eval4/llama3-8b-5shot_metrics.json',\n",
    "    'llama3-70b-5shot': 'data/ft-v4.1/eval4/llama3-70b-5shot_metrics.json',\n",
    "    'gpt3.5-0shot': 'data/ft-v4.1/eval4/gpt-3.5-0shot_metrics.json',\n",
    "    'gpt3.5-5shot': 'data/ft-v4.1/eval4/gpt-3.5-5shot_metrics.json',\n",
    "    # 'mistral-7b-0shot': 'data/ft-v4.1/eval4/mistral-7b-0shot_metrics.json',\n",
    "    # 'mistral-7b-20p-lora': 'data/ft-v4.1/eval4/mistral7b_v4.1_20p_lora_a256r512_metrics.json',\n",
    "}\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for model_name, path in metrics_files.items():\n",
    "    with open(path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            \n",
    "            ex = json.loads(line)\n",
    "            \n",
    "            for metric_name, metric_score in ex['metrics'].items():\n",
    "                cur_dict = {}\n",
    "                cur_dict['metric'] = metric_name\n",
    "                cur_dict['score'] = metric_score\n",
    "                cur_dict['model'] = model_name\n",
    "                cur_dict['query'] = ex['query']\n",
    "                cur_dict['response'] = ex['response']\n",
    "                cur_dict['split'] = ex['split']\n",
    "                df_list.append(cur_dict)\n",
    "\n",
    "\n",
    "len(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a5b028-4655-424e-ac6f-4a2541f5bafe",
   "metadata": {},
   "source": [
    "## compare metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472ddc53-fdb7-4aa3-9f8b-8e4854f984e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_list)\n",
    "\n",
    "df = df[df.split == 'instruction']\n",
    "df = df[df.metric.str.contains('helpfulness')]\n",
    "print(len(df))\n",
    "sns.boxplot(data=df, x='metric', y='score', hue='model')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"G-Eval helpfulness score of different models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61907e2-4d2e-46a3-b06c-99d6a7e8e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_list)\n",
    "\n",
    "df = df[df.split == 'non_compliant']\n",
    "df = df[df.metric.str.contains('safety')]\n",
    "print(len(df))\n",
    "sns.boxplot(data=df, x='metric', y='score', hue='model')\n",
    "plt.title(\"G-Eval safety score of different models\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882a722-9841-413e-9682-b43ec8b54083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_list)\n",
    "\n",
    "df = df[df.split == 'non_compliant']\n",
    "df = df[df.metric.str.contains('helpfulness')]\n",
    "print(len(df))\n",
    "sns.boxplot(data=df, x='metric', y='score', hue='model')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501673d6-cbab-4b13-8927-02b62d9da7aa",
   "metadata": {},
   "source": [
    "## head to head comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2636e67-c6d5-438f-8d43-91e6c4b35afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "def get_head_to_head_scores(metrics_files, metric_name='helpfulness_with_ref (GEval)', only_keep_splits=None, th=0.01):\n",
    "    wins = {model: 0 for model in metrics_files.keys()}\n",
    "    wins['tie'] = 0\n",
    "    model_metrics = {model: [] for model in metrics_files.keys()}\n",
    "    \n",
    "    for model_name, path in metrics_files.items():\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                ex = json.loads(line)\n",
    "                if only_keep_splits is not None and ex['split'] not in only_keep_splits:\n",
    "                    continue\n",
    "                model_metrics[model_name].append(ex)\n",
    "    \n",
    "    total_examples = len(model_metrics[list(model_metrics.keys())[0]])\n",
    "    model_names = list(model_metrics.keys())\n",
    "    \n",
    "    pairwise_results = {}\n",
    "    \n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        pairwise_results[(model1, model2)] = {model1: 0, model2: 0, 'tie': 0}\n",
    "        for i in range(total_examples):\n",
    "            model1_score = model_metrics[model1][i]['metrics'][metric_name]\n",
    "            model2_score = model_metrics[model2][i]['metrics'][metric_name]\n",
    "\n",
    "            if model1_score is None or model2_score is None:\n",
    "                # print(\"Incomplete scores detected...\")\n",
    "                continue\n",
    "\n",
    "            if abs(model1_score - model2_score) < th:\n",
    "                pairwise_results [(model1, model2)]['tie'] += 1\n",
    "            elif model1_score > model2_score:\n",
    "                pairwise_results[(model1, model2)][model1] += 1\n",
    "            elif model2_score > model1_score:\n",
    "                pairwise_results[(model1, model2)][model2] += 1\n",
    "    \n",
    "    return pairwise_results\n",
    "\n",
    "\n",
    "def plot_pairwise_head_to_head_scores(metric_files, metric_name='helpfulness_with_ref (GEval)', only_keep_splits=None, threshold=0.1):\n",
    "    pairwise_scores = get_head_to_head_scores(metric_files, metric_name=metric_name, only_keep_splits=only_keep_splits, th=threshold)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    models = list(metrics_files.keys())\n",
    "    num_models = len(models)\n",
    "    win_matrix = np.zeros((num_models, num_models))\n",
    "    \n",
    "    model_index = {model: idx for idx, model in enumerate(models)}\n",
    "    \n",
    "    for (model1, model2), scores in pairwise_scores.items():\n",
    "        total = scores[model1] + scores[model2] + scores['tie']\n",
    "        print(f\"total comparison samples for {(model1, model2)}: {total}\")\n",
    "        win_matrix[model_index[model1], model_index[model2]] = scores[model1] / total\n",
    "        win_matrix[model_index[model2], model_index[model1]] = scores[model2] / total\n",
    "    \n",
    "    # Plotting the pairwise win rate matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(win_matrix, cmap='RdYlGn')\n",
    "    \n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(num_models))\n",
    "    ax.set_yticks(np.arange(num_models))\n",
    "    ax.set_xticklabels(models, rotation=90)\n",
    "    ax.set_yticklabels(models)\n",
    "    \n",
    "    # Display the win rates\n",
    "    for i in range(num_models):\n",
    "        for j in range(num_models):\n",
    "            ax.text(j, i, f'{win_matrix[i, j]:.2f}', ha='center', va='center', color='black')\n",
    "    \n",
    "    # Color bar\n",
    "    plt.colorbar(cax)\n",
    "    \n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Model')\n",
    "    plt.title(f'Pairwise Win Rate According to {metric_name}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_pairwise_head_to_head_scores(metrics_files, metric_name='helpfulness_with_ref (GEval)', only_keep_splits=['instruction'], threshold=0.01)\n",
    "plot_pairwise_head_to_head_scores(metrics_files, metric_name='helpfulness_without_ref (GEval)', only_keep_splits=['instruction'], threshold=0.01)\n",
    "plot_pairwise_head_to_head_scores(metrics_files, metric_name='safety_with_ref (GEval)', only_keep_splits=['non_compliant'], threshold=0.01)\n",
    "plot_pairwise_head_to_head_scores(metrics_files, metric_name='safety_without_ref (GEval)', only_keep_splits=['non_compliant'], threshold=0.01)\n",
    "plot_pairwise_head_to_head_scores(metrics_files, metric_name='helpfulness_without_ref (GEval)', only_keep_splits=['non_compliant'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
